{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Style Transfer CNNs\n",
    "By David Tarazi and Dieter Brehm, October 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Import libraries and a local utility script for the sake of shortening and simplifying the code shown in this report notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MLUtility as util\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "if tf.__version__ != \"1.14.0\":\n",
    "    print(\"Warning, tensorflow version not compatible with this notebook\")\n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show live results if running TF -> 1.14\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download two test images\n",
    "#content_path = tf.keras.utils.get_file('olin.jpg','https://github.com/Inkering/ML-Project-1/blob/master/71934026_761426527628858_7267766954746707968_n.png?raw=true')\n",
    "#style_path = tf.keras.utils.get_file('painting.jpg','https://github.com/Inkering/ML-Project-1/raw/master/StarryNight.jpg')\n",
    "content_path = \"Data/olin.png\"\n",
    "style_path = \"Data/city.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the two inputs\n",
    "Quickly visualize the two images we will applying filters onto, i.e. the style image we will be getting style information from and the content image we will be applying a generated convolution filter onto to make it look like the style of the style image. For now, we'll look at the two unmodified inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = util.transform_image(content_path, 512)\n",
    "style_image = util.transform_image(style_path, 512)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(13,8)\n",
    "plt.subplot(1, 2, 1)\n",
    "util.imshow(content_image, 'Content Image')\n",
    "plt.subplot(1, 2, 2)\n",
    "util.imshow(style_image, 'Style Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let Keras preprocess the input to fit desired dimensionality\n",
    "Next, we'll use Keras to preprocess the input image to work with the vgg model we will transfer learning with. The process involves scaling it into a square and increasing the pixel values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: should we tune the 200 brightness variable and check results?**  \n",
    "**Q: why do we adjust brightness at all? does it work without it?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.applications.vgg19.preprocess_input(content_image*200)\n",
    "x = tf.image.resize(x, (224, 224))\n",
    "util.imshow(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Cool) What does the VGG model think the input looks like?\n",
    "We can quickly run a classification on the input, after all, that is what vgg is actually for despite the fact that we are essentially intercepting the intermediary layers before a squashing and classification occurs. It is interesting to see what it thinks the olin O is though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download Keras\n",
    "vgg = tf.keras.applications.VGG19(include_top=True, weights='imagenet')\n",
    "prediction_probabilities = vgg(x)\n",
    "# show the top 5 predictions\n",
    "predicted = tf.keras.applications.vgg19.decode_predictions(prediction_probabilities.numpy())[0]\n",
    "[(class_name, prob) for (number, class_name, prob) in predicted]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing transfer learning and style content loss\n",
    "**A note on how the loss function works in this implementation**:  \n",
    "Basically, the content loss function is calculating the sum of the mean squared\n",
    "error between the generated image and the content image. The activations in the\n",
    "higher layers of the generated image often represent objects shown in the\n",
    "image. So the content loss function focuses on the higher layers correlated\n",
    "specifically to objects or content.\n",
    "\n",
    "On the other hand, the style loss function looks at all of the layers in the\n",
    "CNN. style information is measured as the amount of correlation present between\n",
    "features maps in a given layer. Next, a loss is defined as the difference of\n",
    "correlation present between the feature maps computed by the generated image and\n",
    "the style image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll pick layers to work with for the content and style representations:\n",
    "\n",
    "**Q**: How does the number of style layers chosen, and which content layer chosen, change the algorithm output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layers = ['block5_conv2'] \n",
    "\n",
    "style_layers = ['block1_conv1',\n",
    "                'block2_conv1',\n",
    "                'block3_conv1', \n",
    "                'block4_conv1', \n",
    "                'block5_conv1']\n",
    "\n",
    "num_content_layers = len(content_layers)\n",
    "num_style_layers = len(style_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, according to idiosyncrasies of the model format, we need to create a new version of the model which involves feeding only the desired layers we wrote above of the VGG classifier network into a new model which only considers those layers. After creating this limited model compared to the massive VGG classifier system, we can apply the style image to it and get some outputs for those given layers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_model = util.vgg_2_functional_model(style_layers)\n",
    "style_outputs = style_model(style_image*255)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a set of outputs, but we need a method to combine them so that we can quantitatively compare it to the content and compute a loss which we can later perform gradient descent on. One method is to use a gram matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(input_tensor):\n",
    "    # Finds the correlation matrix for style loss and sums all of the feature correlations for a particular layer\n",
    "    result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
    "    input_shape = tf.shape(input_tensor)\n",
    "    # num_locations is the same as number of pixels\n",
    "    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
    "    # normalize\n",
    "    return result/(num_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our utilities script, we define a shortcut class that allows us to repeatedly call a function that generates style and content outputs. It utilizes the gram matrix to combine the outputs of the style layers. It returns both that combined style output matrix and the outputs given from passing the content image through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the class described above\n",
    "model_instance = util.StyleContentModel(style_layers, content_layers)\n",
    "\n",
    "results = model_instance.call(tf.constant(content_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing a intermediary style layer\n",
    "Each style layer is a tensor that holds style information as a gram matrix. We can look at a visual representation of it and examine its shape!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pluck out a layer and convert to np array\n",
    "layer_shown = results[\"style\"][\"block1_conv1\"]\n",
    "print(layer_shown.shape)\n",
    "layer_shown = layer_shown*255\n",
    "layer_shown = np.array(layer_shown, dtype=np.uint8)\n",
    "\n",
    "#reshape to an image size\n",
    "layer_shown = np.reshape(layer_shown,(8,8,64))\n",
    "\n",
    "print(layer_shown.shape)\n",
    "plt.imshow(layer_shown[:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the experiment\n",
    "Now, we can run a gradient descent and get a significant style transfer output! Firstly, by setting the targets we will run our functions against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the targets by pulling them out of the run model instance\n",
    "style_targets = model_instance(style_image)['style']\n",
    "content_targets = model_instance(content_image)['content']\n",
    "\n",
    "# Defining the image as a tensorflow variable to make it constant\n",
    "image = tf.Variable(content_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the optimizer - we can tune the learning rate and beta value (decay rate for first moment estimates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.train.AdamOptimizer(learning_rate=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with all gradient descents, we need to choose a starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_weight=1e-2\n",
    "content_weight=1e4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: should we include the loss function or is the explanation and maybe some latex sufficient?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = util.experiment_handler(content_image,\n",
    "                                     model_instance,\n",
    "                                     style_targets,\n",
    "                                     content_targets,\n",
    "                                     style_weight,\n",
    "                                     content_weight,\n",
    "                                     num_style_layers,\n",
    "                                     num_content_layers)\n",
    "experiment.run(10,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the loss over experiment runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(experiment.losses)\n",
    "plt.title(\"Gradient Descent loss versus run for style transfer\")\n",
    "plt.xlabel(\"Run Number\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
